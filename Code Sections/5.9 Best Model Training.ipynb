{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadbadi/Clustering_Frequency/blob/main/Code%20Sections/5.9%20Best%20Model%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c81a8ec",
      "metadata": {
        "id": "7c81a8ec",
        "papermill": {
          "duration": 0.001708,
          "end_time": "2025-03-18T05:58:24.214762",
          "exception": false,
          "start_time": "2025-03-18T05:58:24.213054",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### **5.9 Best Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394c322c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-18T05:58:24.219843Z",
          "iopub.status.busy": "2025-03-18T05:58:24.219456Z",
          "iopub.status.idle": "2025-03-18T06:14:17.846836Z",
          "shell.execute_reply": "2025-03-18T06:14:17.845742Z"
        },
        "id": "394c322c",
        "outputId": "5eb5b7f9-63a9-4601-b7cb-3bd8601e2449",
        "papermill": {
          "duration": 953.632046,
          "end_time": "2025-03-18T06:14:17.848590",
          "exception": false,
          "start_time": "2025-03-18T05:58:24.216544",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"color: black; font-size: 16px; font-weight: bold;\">\n",
              "    Checking input files...<br>\n",
              "    Missing values in original_data: <span style=\"color: green;\">0</span><br>\n",
              "    Missing values in feature_combos: <span style=\"color: green;\">0</span>\n",
              "</p>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: black; font-size: 16px; font-weight: bold;\">Checking feature set validity.<br><span style=\"color: darkblue;\">34</span>: <span style=\"color: green;\">4</span> valid features out of <span style=\"color: green;\">4</span><br><span style=\"color: darkblue;\">35</span>: <span style=\"color: green;\">4</span> valid features out of <span style=\"color: green;\">4</span><br><span style=\"color: darkblue;\">36</span>: <span style=\"color: green;\">4</span> valid features out of <span style=\"color: green;\">4</span><br><span style=\"color: darkblue;\">41</span>: <span style=\"color: green;\">4</span> valid features out of <span style=\"color: green;\">4</span><br><span style=\"color: darkblue;\">45</span>: <span style=\"color: green;\">4</span> valid features out of <span style=\"color: green;\">4</span><br></p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: black; font-size: 16px; font-weight: bold;\">Standardization Checks:<br> Scaling successful for <span style=\"color: darkblue;\">34</span> (<span style=\"color: green;\">4</span> numerical features).<br> Scaling successful for <span style=\"color: darkblue;\">35</span> (<span style=\"color: green;\">4</span> numerical features).<br> Scaling successful for <span style=\"color: darkblue;\">36</span> (<span style=\"color: green;\">4</span> numerical features).<br> Scaling successful for <span style=\"color: darkblue;\">41</span> (<span style=\"color: green;\">4</span> numerical features).<br> Scaling successful for <span style=\"color: darkblue;\">45</span> (<span style=\"color: green;\">4</span> numerical features).<br></p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"color: black; font-size: 16px; font-weight: bold;\">\n",
              "    Checking _id mapping...<br>\n",
              "    Total unique IDs: <span style=\"color: green;\">54311</span>, Duplicates: <span style=\"color: green;\">0</span>\n",
              "</p>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: darkblue; font-size: 16px; font-weight: bold;\">Pre-clustering checks completed. Data is ready for clustering!</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast                                                                        # For safely evaluating strings\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/Clustering_Frequency/refs/heads/main/Output_CSV/FE_Encoded_New.csv\"  # Read the dataset from CSV file\n",
        "url1 = \"https://raw.githubusercontent.com/mohammadbadi/Clustering_Frequency/refs/heads/main/Output_CSV/Feature_Combo_New_Results.csv\"  # Load feature combinations\n",
        "original_data = pd.read_csv(url)\n",
        "feature_combos = pd.read_csv(url1)\n",
        "\n",
        "# Debugging: Check input files for missing values\n",
        "html_output = \"\"\"\n",
        "<p style=\"color: black; font-size: 16px; font-weight: bold;\">\n",
        "    Checking input files...<br>\n",
        "    Missing values in original_data: <span style=\"color: green;\">{orig_missing}</span><br>\n",
        "    Missing values in feature_combos: <span style=\"color: green;\">{feat_missing}</span>\n",
        "</p>\n",
        "\"\"\"\n",
        "display(HTML(html_output.format(orig_missing=original_data.isnull().sum().sum(), feat_missing=feature_combos.isnull().sum().sum())))\n",
        "\n",
        "original_data['_id'] = original_data.index                                        # Store _id before clustering\n",
        "sample_data = original_data.copy()\n",
        "# sample_data = original_data.sample(frac=0.1, random_state=42)                   # Use 10% of the data (for now)\n",
        "\n",
        "# Define feature sets using the new column names\n",
        "set_names = ['34', '35', '36', '41', '45']      # Define the set numbers to match\n",
        "feature_sets = []                              # Initialize an empty list to store feature sets\n",
        "for set_name in set_names:\n",
        "    # Now using \"Set Number\" and \"Features\" columns from feature_combos\n",
        "    matched_features = feature_combos[feature_combos['Set Number'] == int(set_name)]['Features']\n",
        "    if not matched_features.empty:\n",
        "        features_list = ast.literal_eval(matched_features.values[0])              # Convert string to list\n",
        "        feature_sets.append(features_list)\n",
        "    else:\n",
        "        feature_sets.append([])                                                   # Handle missing feature sets\n",
        "\n",
        "# Debugging: Check feature set validity\n",
        "debug_feature = \"<p style=\\\"color: black; font-size: 16px; font-weight: bold;\\\">Checking feature set validity.<br>\"\n",
        "for i, features in enumerate(feature_sets, start=1):\n",
        "    valid_features = [f for f in features if f in sample_data.columns]\n",
        "    debug_feature += f\"<span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span>: <span style=\\\"color: green;\\\">{len(valid_features)}</span> valid features out of <span style=\\\"color: green;\\\">{len(features)}</span><br>\"\n",
        "    if len(valid_features) == 0:\n",
        "        debug_feature += f\"Warning: Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> has no valid features!<br>\"\n",
        "debug_feature += \"</p>\"\n",
        "display(HTML(debug_feature))\n",
        "\n",
        "# Debugging: Standardization Check for each feature set\n",
        "debug_standard = \"<p style=\\\"color: black; font-size: 16px; font-weight: bold;\\\">Standardization Checks:<br>\"\n",
        "for i, features in enumerate(feature_sets, start=1):\n",
        "    valid_features = [f for f in features if f in sample_data.columns]\n",
        "    if valid_features:\n",
        "        data_for_clustering = sample_data[valid_features].copy()\n",
        "        numerical_cols = data_for_clustering.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "        if len(numerical_cols) == 0:\n",
        "            debug_standard += f\" Warning: Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> has NO numerical columns for clustering!<br>\"\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                scaled_data = scaler.fit_transform(data_for_clustering[numerical_cols])\n",
        "                debug_standard += f\" Scaling successful for <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> (<span style=\\\"color: green;\\\">{len(numerical_cols)}</span> numerical features).<br>\"\n",
        "            except Exception as e:\n",
        "                debug_standard += f\" Error scaling <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span>: {e}<br>\"\n",
        "debug_standard += \"</p>\"\n",
        "display(HTML(debug_standard))\n",
        "\n",
        "# Debugging: Check _id mapping\n",
        "sample_ids = sample_data['_id'].values                                            # Store _id for mapping\n",
        "duplicated_ids = sample_data['_id'].duplicated().sum()\n",
        "html_id = \"\"\"\n",
        "<p style=\"color: black; font-size: 16px; font-weight: bold;\">\n",
        "    Checking _id mapping...<br>\n",
        "    Total unique IDs: <span style=\"color: green;\">{unique}</span>, Duplicates: <span style=\"color: green;\">{dups}</span>\n",
        "</p>\n",
        "\"\"\"\n",
        "display(HTML(html_id.format(unique=len(set(sample_ids)), dups=duplicated_ids)))\n",
        "if duplicated_ids > 0:\n",
        "    display(HTML(\"<p style=\\\"color: red; font-size: 16px; font-weight: bold;\\\">Warning: Duplicate _id values found!</p>\"))\n",
        "\n",
        "display(HTML(\"<p style=\\\"color: darkblue; font-size: 16px; font-weight: bold;\\\">Pre-clustering checks completed. Data is ready for clustering!</p>\"))\n",
        "\n",
        "# Prepare clustering results DataFrame\n",
        "clustering_results = original_data.copy()                                         # Create a copy to store clustering results\n",
        "for i in range(1, 6):                                                             # Add placeholder columns for clustering results for each set\n",
        "    for algo in ['KMeans','DBSCAN']:\n",
        "        clustering_results[f'{algo}{i}_Cluster'] = -1\n",
        "        clustering_results[f'{algo}{i}_Silhouette_Score'] = np.nan\n",
        "        clustering_results[f'{algo}{i}_Davies_Bouldin_Index'] = np.nan\n",
        "        if algo == \"KMeans\":\n",
        "            clustering_results[f'{algo}{i}_Calinski_Harabasz_Score'] = np.nan\n",
        "        clustering_results[f'{algo}{i}_Prediction_Accuracy'] = np.nan\n",
        "\n",
        "# Clustering for each feature set with debugging outputs\n",
        "debug_cluster = \"<p style=\\\"color: darkblue; font-size: 18px; font-weight: bold;\\\">Clustering Debug Info:<br>\"\n",
        "for i, features in enumerate(feature_sets, start=1):\n",
        "    valid_features = [f for f in features if f in sample_data.columns]\n",
        "    data_for_clustering = sample_data[valid_features].copy()\n",
        "    sample_ids = sample_data['_id'].values                                        # Store _id for mapping back\n",
        "    numerical_cols = data_for_clustering.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data_for_clustering[numerical_cols]), columns=numerical_cols)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42)                                # KMeans Clustering\n",
        "    kmeans_labels = kmeans.fit_predict(data_scaled)\n",
        "    silhouette_score_kmeans = silhouette_score(data_scaled, kmeans_labels)\n",
        "    davies_bouldin_score_kmeans = davies_bouldin_score(data_scaled, kmeans_labels)\n",
        "    calinski_harabasz_score_kmeans = calinski_harabasz_score(data_scaled, kmeans_labels)\n",
        "    kmeans_accuracy = max(0, silhouette_score_kmeans) * 100\n",
        "\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)                                       # DBSCAN Clustering\n",
        "    dbscan_labels = dbscan.fit_predict(data_scaled)\n",
        "    silhouette_score_dbscan = -1 if len(set(dbscan_labels)) <= 1 else silhouette_score(data_scaled, dbscan_labels)\n",
        "    davies_bouldin_score_dbscan = -1 if len(set(dbscan_labels)) <= 1 else davies_bouldin_score(data_scaled, dbscan_labels)\n",
        "    dbscan_accuracy = max(0, silhouette_score_dbscan) * 100\n",
        "\n",
        "    debug_cluster += f\"Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> - KMeans labels: <span style=\\\"color: green;\\\">{kmeans_labels[:10]}</span> ...<br>\"\n",
        "    debug_cluster += f\"Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> - DBSCAN labels: <span style=\\\"color: green;\\\">{dbscan_labels[:10]}</span> ...<br>\"\n",
        "\n",
        "    for idx, original_idx in enumerate(sample_ids):\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Cluster'] = kmeans_labels[idx]\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Silhouette_Score'] = silhouette_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Davies_Bouldin_Index'] = davies_bouldin_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Calinski_Harabasz_Score'] = calinski_harabasz_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Prediction_Accuracy'] = kmeans_accuracy\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Cluster'] = dbscan_labels[idx]\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Silhouette_Score'] = silhouette_score_dbscan\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Davies_Bouldin_Index'] = davies_bouldin_score_dbscan\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Prediction_Accuracy'] = dbscan_accuracy\n",
        "\n",
        "debug_cluster += \"</p>\"\n",
        "display(HTML(debug_cluster))\n",
        "\n",
        "# Debugging: Check for NaN values in clustering_results and print a few rows\n",
        "debug_nan = \"<ul style=\\\"color: darkblue; font-size: 18px; font-weight: bold;\\\"><li>Checking for NaN values in clustering_results: <span style=\\\"color: green;\\\">{nan_dict}</span></li><li>Preview of clustering_results (first 10 rows): <span style=\\\"color: green;\\\">{preview}</span></li></ul>\"\n",
        "nan_dict = clustering_results.isnull().sum().to_dict()\n",
        "preview = clustering_results.head(10).to_html(classes=\"table table-bordered\", index=False)\n",
        "display(HTML(debug_nan.format(nan_dict=nan_dict, preview=preview)))\n",
        "\n",
        "clustering_results.to_csv('New_Best_Clustering_Models.csv', index=False)\n",
        "display(HTML(\"\"\"\n",
        "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "         Clustering results saved as <span style=\"color: darkblue;\">'clustering_results.csv'</span>.\n",
        "    </p>\n",
        "\"\"\"))\n",
        "\n",
        "# Define the base columns (first file)\n",
        "base_columns = ['_id', 'EVENT_UNIQUE_ID', 'OCC_YEAR', 'OCC_MONTH', 'OCC_DAY', 'OCC_DOY', 'OCC_DOW', 'OCC_HOUR', 'DIVISION', 'LOCATION_TYPE', 'PREMISES_TYPE', 'HOOD_158', 'NEIGHBOURHOOD_158', 'LONG_WGS84', 'LAT_WGS84', 'OCC_DATETIME', 'reporting_delay_days', 'reporting_delay_hours', 'Location_Engineered', 'Hood_158_Encoded', 'Division_Encoded', 'Location_Engineered_Other', 'Location_Engineered_Public', 'Location_Engineered_Residential', 'OCC_Month_Encoded', 'OCC_DOW_Encoded']\n",
        "clustering_results_base = clustering_results[base_columns]\n",
        "clustering_results_base.to_csv('New_Clustering_Base_Features.csv', index=False)\n",
        "\n",
        "# Define the clustering statistics columns (second file)\n",
        "stats_columns = ['_id',\n",
        "                 'KMeans1_Cluster', 'KMeans1_Silhouette_Score', 'KMeans1_Davies_Bouldin_Index', 'KMeans1_Calinski_Harabasz_Score', 'KMeans1_Prediction_Accuracy',\n",
        "                 'DBSCAN1_Cluster', 'DBSCAN1_Silhouette_Score', 'DBSCAN1_Davies_Bouldin_Index', 'DBSCAN1_Prediction_Accuracy',\n",
        "                 'KMeans2_Cluster', 'KMeans2_Silhouette_Score', 'KMeans2_Davies_Bouldin_Index', 'KMeans2_Calinski_Harabasz_Score', 'KMeans2_Prediction_Accuracy',\n",
        "                 'DBSCAN2_Cluster', 'DBSCAN2_Silhouette_Score', 'DBSCAN2_Davies_Bouldin_Index', 'DBSCAN2_Prediction_Accuracy',\n",
        "                 'KMeans3_Cluster', 'KMeans3_Silhouette_Score', 'KMeans3_Davies_Bouldin_Index', 'KMeans3_Calinski_Harabasz_Score', 'KMeans3_Prediction_Accuracy',\n",
        "                 'DBSCAN3_Cluster', 'DBSCAN3_Silhouette_Score', 'DBSCAN3_Davies_Bouldin_Index', 'DBSCAN3_Prediction_Accuracy',\n",
        "                 'KMeans4_Cluster', 'KMeans4_Silhouette_Score', 'KMeans4_Davies_Bouldin_Index', 'KMeans4_Calinski_Harabasz_Score', 'KMeans4_Prediction_Accuracy',\n",
        "                 'DBSCAN4_Cluster', 'DBSCAN4_Silhouette_Score', 'DBSCAN4_Davies_Bouldin_Index', 'DBSCAN4_Prediction_Accuracy',\n",
        "                 'KMeans5_Cluster', 'KMeans5_Silhouette_Score', 'KMeans5_Davies_Bouldin_Index', 'KMeans5_Calinski_Harabasz_Score', 'KMeans5_Prediction_Accuracy',\n",
        "                 'DBSCAN5_Cluster', 'DBSCAN5_Silhouette_Score', 'DBSCAN5_Davies_Bouldin_Index', 'DBSCAN5_Prediction_Accuracy']\n",
        "clustering_results_stats = clustering_results[stats_columns]\n",
        "clustering_results_stats.to_csv('New_Clustering_Result_Stats.csv', index=False)\n",
        "\n",
        "# Download each file only once\n",
        "files.download('New_Clustering_Result_Stats.csv')\n",
        "files.download('New_Best_Clustering_Models.csv')\n",
        "files.download('New_Clustering_Base_Features.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6888578,
          "sourceId": 11056653,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 958.009718,
      "end_time": "2025-03-18T06:14:19.374702",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-03-18T05:58:21.364984",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}